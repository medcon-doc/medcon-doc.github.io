<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <title>University of Mannheim | Mediated Contestation in Comparative Perspective project | Documentation - Content Analysis - Reliabilities</title>

        <meta name="viewport" content="width=device-width, initial-scale=1">
		
		<link rel="stylesheet" type="text/css" href="../../../common/bootstrap.min.css"/>
		<link rel="stylesheet" type="text/css" href="../../../common/medcon.css"/>
		
		<link href="../../../common/featherlight.min.css" type="text/css" rel="stylesheet" />
		<script src="../../../common/jquery-latest.js"></script>
		<script src="../../../common/featherlight.min.js" type="text/javascript" charset="utf-8"></script>
    </head>
    <body>
		<div id="header"></div>
	
		<div class="container-fluid">
			<div class="row">
				<div class="col-sm-2">
					<div id="subNavbar"></div>
				</div>
				<div class="col-sm-10">
					<h3>Final Inter-Coder Reliability</h3>
					<p>
						Each news item in the dataset was coded by two coders (see <a href="../coding-procedure" data-role="internal">coding procedure</a>). After each of the two coders coded the material, they held a consensus discussion in order to resolve dissenting codes. The <a href="../data-sheets" data-role="internal">final dataset</a> created for the statistical analysis consists of the codes after the consensus discussion.
					</p>
					<p>
						To assess the coding quality, we extracted the codes of the individual coders before the consensus discussion and calculated the interrater agreement. It was calculated based on the framework presented by Klein (2018). The next table contains the results of the reliability analysis for each of the observed variables. The material needed for reproduction of the reliability calculation is attached below.
					</p>
					<table class="table table-condensed table-striped" style="width: auto;">
					<thead>
						<tr>
							<th></th>
							<th class="text-center" style="width: 16%">Tone (item level)</th>
							<th class="text-center" style="width: 16%">Opposing positions (item level)</th>
							<th class="text-center" style="width: 16%">Civil society, citizen, expert presence (actor level)</th>
							<th class="text-center" style="width: 16%">Opposition speaker presence (actor level)</th>
							<th class="text-center" style="width: 16%">Religious multiperspectivalness (actor level)</th>
	
						</tr>
					</thead>
					<tbody>
						<tr>
							<td>Percent Agreement</td>
							<td class="text-center">.94</td>
							<td class="text-center">.91</td>
							<td class="text-center">.88</td>
							<td class="text-center">.92</td>
							<td class="text-center">.92</td>

						</tr>
						<tr>
							<td>Brennan and Prediger's Kappa</td>
							<td class="text-center">.92</td>
							<td class="text-center">.86</td>
							<td class="text-center">.87</td>
							<td class="text-center">.91</td>
							<td class="text-center">.92</td>

						</tr>
						<tr>
							<td>Cohen/Conger's Kappa</td>
							<td class="text-center">.74</td>
							<td class="text-center">.82</td>
							<td class="text-center">.85</td>
							<td class="text-center">.79</td>
							<td class="text-center">.82</td>

						</tr>
						<tr>
							<td>Scott/Fleiss' Pi</td>
							<td class="text-center">.74</td>
							<td class="text-center">.82</td>
							<td class="text-center">.85</td>
							<td class="text-center">.79</td>
							<td class="text-center">.82</td>

						</tr>
						<tr>
							<td>Gwet's AC</td>
							<td class="text-center">.94</td>
							<td class="text-center">.88</td>
							<td class="text-center">.87</td>
							<td class="text-center">.91</td>
							<td class="text-center">.92</td>

						</tr>
						<tr>
							<td>Krippendorff's Alpha</td>
							<td class="text-center">.74</td>
							<td class="text-center">.82</td>
							<td class="text-center">.85</td>
							<td class="text-center">.79</td>
							<td class="text-center">.82</td>

						</tr>
						<tr>
							<td>N</td>
							<td class="text-center">1,700</td>
							<td class="text-center">1,699</td>
							<td class="text-center">10,968</td>
							<td class="text-center">10,968</td>
							<td class="text-center">10,968</td>

						</tr>
					</tbody>
					</table>
					<p>
					<i>Note</i>: Reliability analyses for all variables included first and second coder during main stage of content analysis. The constructs civil society presence, citizen presence, and expert presence were captured by a single variable (see codebook). All estimates are unweighted.
					</p>
					<table class="table table-condensed table-striped" style="width: auto;">
					<thead>
						<tr>
							<th></th>
							<th class="text-center" style="width: 16%">Common good reference (justification level)</th>
							<th class="text-center" style="width: 16%">Out-group reference (justification level)</th>
							<th class="text-center" style="width: 16%">In-group reference (justification level)</th>
							<th class="text-center" style="width: 16%">DQI index [in-group, out-group, common good reference] (justification level)</th>
							
						</tr>
					</thead>
					<tbody>
						<tr>
							<td>Percent Agreement</td>
							<td class="text-center">.92</td>
							<td class="text-center">.90</td>
							<td class="text-center">.96</td>
							<td class="text-center">.92</td>
						</tr>
						<tr>
							<td>Brennan and Prediger's Kappa</td>
							<td class="text-center">.85</td>
							<td class="text-center">.81</td>
							<td class="text-center">.91</td>
							<td class="text-center">.73</td>
						</tr>
						<tr>
							<td>Cohen/Conger's Kappa</td>
							<td class="text-center">.53</td>
							<td class="text-center">.62</td>
							<td class="text-center">.58</td>
							<td class="text-center">.61</td>
						</tr>
						<tr>
							<td>Scott/Fleiss' Pi</td>
							<td class="text-center">.53</td>
							<td class="text-center">.62</td>
							<td class="text-center">.58</td>
							<td class="text-center">.61</td>
							
						</tr>
						<tr>
							<td>Gwet's AC</td>
							<td class="text-center">.91</td>
							<td class="text-center">.87</td>
							<td class="text-center">.95</td>
							<td class="text-center">.86</td>
							
						</tr>
						<tr>
							<td>Krippendorff's Alpha</td>
							<td class="text-center">.53</td>
							<td class="text-center">.62</td>
							<td class="text-center">.58</td>
							<td class="text-center">.61</td>
						</tr>
						<tr>
							<td>N</td>
							<td class="text-center">4,867</td>
							<td class="text-center">4,867</td>
							<td class="text-center">4,867</td>
							<td class="text-center">4,867</td>
						</tr>
					</tbody>
					</table>
					<p>
					<i>Note</i>: Reliability analyses for all variables included first and second coder during main stage of content analysis. The estimates for DQI index are based on ordinal weighting as proposed by Gwet (2014, pp. 91-92).
					</p>
					<table class="table table-condensed table-striped" style="width: auto;">
					<thead>
						<tr>
							<th></th>
							<th class="text-center" style="width: 16%">Valence (actor reference level)</th>
							<th class="text-center" style="width: 16%">Recognition (actor reference level)</th>
							<th class="text-center" style="width: 16%">Outrage (actor reference level)</th>
							<th class="text-center" style="width: 16%">Responsiveness (actor reference level)</th>
							
							
						</tr>
					</thead>
					<tbody>
						<tr>
							<td>Percent Agreement</td>
							<td class="text-center">.84</td>
							<td class="text-center">.97</td>
							<td class="text-center">.98</td>
							<td class="text-center">.95</td>
							
						</tr>
						<tr>
							<td>Brennan and Prediger's Kappa</td>
							<td class="text-center">.78</td>
							<td class="text-center">.94</td>
							<td class="text-center">.95</td>
							<td class="text-center">.92</td>
							
						</tr>
						<tr>
							<td>Cohen/Conger's Kappa</td>
							<td class="text-center">.74</td>
							<td class="text-center">.69</td>
							<td class="text-center">.77</td>
							<td class="text-center">.68</td>
							
						</tr>
						<tr>
							<td>Scott/Fleiss' Pi</td>
							<td class="text-center">.74</td>
							<td class="text-center">.65</td>
							<td class="text-center">.74</td>
							<td class="text-center">.71</td>
							
						</tr>
						<tr>
							<td>Gwet's AC</td>
							<td class="text-center">.79</td>
							<td class="text-center">.97</td>
							<td class="text-center">.97</td>
							<td class="text-center">.94</td>
							
						</tr>
						<tr>
							<td>Krippendorff's Alpha</td>
							<td class="text-center">.75</td>
							<td class="text-center">.71</td>
							<td class="text-center">.79</td>
							<td class="text-center">.64</td>
							
						</tr>
						<tr>
							<td>N</td>
							<td class="text-center">3,863</td>
							<td class="text-center">3,863</td>
							<td class="text-center">3,863</td>
							<td class="text-center">3,864</td>
													</tr>
					</tbody>
					</table>
					
					<p>
					<i>Note</i>: Reliability analyses for all variables included first and second coder during main stage of content analysis. For actor-reference level variables, coders had to code references between actors appearing in the text. A reference is identified by the text, the reference giving actor, and the referred to actor, i.e. there can only be two references for any pair of actors (one in each direction) in a given text.
					</p>
					
					<h4>Benchmark Scale (Landis & Koch, 1977)</h4>
					<table class="table table-condensed table-striped" style="width: auto;">
					<tbody>
					<tr>
						<td>&lt;0.0</td>
						<td>Poor</td>
					</tr>
					<tr>
						<td>0.0 - 0.2</td>
						<td>Slight</td>
					</tr>
					<tr>
						<td>0.2 - 0.4</td>
						<td>Fair</td>
					</tr>
					<tr>
						<td>0.4 - 0.6</td>
						<td>Moderate</td>
					</tr>
					<tr>
						<td>0.6 - 0.8</td>
						<td>Substantial</td>
					</tr>
					<tr>
						<td>0.8 - 1.0</td>
						<td>Almost Perfect</td>
					</tr>
					</tbody>
					</table>
					<h4>Material</h4>
					<p>
						<ul>
							<li><a href="../../files/Appendix - Repro Materials Reliability Content Analysis - Tone.xlsx" download="" target="_blank">Raw Reproduction Data Reliability Content Analysis  - Tone [XLSX]</a></li>
							<li><a href="../../files/Appendix - Repro Materials Reliability Content Analysis - Actor-reference.xlsx" download="" target="_blank">Raw Reproduction Data Reliability Content Analysis  - Actor-reference [XLSX]</a></li>
							<li><a href="../../files/Appendix - Repro Materials Reliability Content Analysis - STATA.do" download="" target="_blank">Reproduction Stata Script Reliability Content Analysis [DO]</a></li>
						</ul>
					</p>
					<h4>References</h4>
					<p>Gwet, K. L. (2014).<i> Handbook of inter-rater reliability: The definitive guide to measuring the extent of agreement among raters</i> (4th ed.). Advanced Analytics, LLC
					   Klein, D. (2018). Implementing a general framework for assessing interrater agreement in Stata. <i>Stata Journal</i>, <i>18</i>(4), 871-901. <a href="https://doi.org/10.1177/1536867X1801800408">https://doi.org/10.1177/1536867X1801800408</a>
					Landis, J. R., & Koch, G. G. (1977). The measurement of observer agreement for categorical data. <i>Biometrics</i>, 33(1), 159â€“174. <a href="https://doi.org/10.2307/2529310">https://doi.org/10.2307/2529310</a></p>

				</div>
				</div>
			</div>
		</div>

		<div id="footer"></div>
		
		<script type="text/javascript">window.linkLevel = 3; window.sectionID = 'reliabilities';</script>
		<script type="text/javascript" src="../../../common/common.js"></script>
		<script type="text/javascript" src='../../subnav.js'></script>
    </body>
</html>
